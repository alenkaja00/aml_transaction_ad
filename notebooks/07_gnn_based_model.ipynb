{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN-Based Outlier Detection Model\n",
    "\n",
    "To set up the environment for the GNN-based outlier detection model, follow these steps to install PyTorch and PyTorch Geometric dependencies:\n",
    "\n",
    "1. Install **PyTorch** with its compatible versions of **torchvision** and **torchaudio** for CUDA 12.4 support:\n",
    "    ```bash\n",
    "    pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "    ```\n",
    "\n",
    "2. Install PyG libraries (`pyg-lib`, `torch_scatter`, `torch_sparse`, `torch_cluster`, and `torch_spline_conv`):\n",
    "    ```bash\n",
    "    pip install pyg-lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
    "    ```\n",
    "\n",
    "3. Install the **torch_geometric** package:\n",
    "    ```bash\n",
    "    pip install torch_geometric\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import subgraph\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# Instantiate PySpark session\n",
    "config = [\n",
    "    (\"spark.driver.memory\", \"64g\"), \n",
    "    (\"spark.executor.memory\", \"64g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"64g\"),\n",
    "    (\"spark.sql.session.timeZone\", \"UTC\")\n",
    "]\n",
    "spark = SparkSession.builder.appName(\"07_gnn_based_model\").config(conf=SparkConf().setAll(config)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = \"HI-Small\"    ## either HI-Small or LI-Small\n",
    "nodes_location = f\"../datasets/synthetic/06_temporal_graph/{DATASET}_nodes\"\n",
    "edges_location = f\"../datasets/synthetic/06_temporal_graph/{DATASET}_edges\"\n",
    "trans_location = f\"../datasets/synthetic/02_preprocessed/{DATASET}-transactions.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read node and edges of temporal graph of sequential transactions\n",
    "nodes = spark.read.parquet(nodes_location)\n",
    "edges = spark.read.parquet(edges_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read all transactions and select transaction_id and amount columns \n",
    "transactions = spark.read.parquet(trans_location)\n",
    "amount_col = transactions.select(\"transaction_id\", \"amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join edges with the amount for the source and for the target\n",
    "edges = edges.join(amount_col, amount_col.transaction_id==edges.src, how='left').drop('transaction_id')\n",
    "edges = edges.withColumnRenamed('amount', 'src_amount')\n",
    "edges = edges.join(amount_col, amount_col.transaction_id==edges.dst, how='left').drop('transaction_id')\n",
    "edges = edges.withColumnRenamed('amount', 'dst_amount')\n",
    "\n",
    "edges = edges.drop('src_date', 'dst_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select flow, fan_in and fan_out edges\n",
    "flow_edges = edges.filter(F.col('edge_type')=='flow').drop('edge_type')\n",
    "fan_in_edges = edges.filter(F.col('edge_type')=='fan_in').drop('edge_type')\n",
    "fan_out_edges = edges.filter(F.col('edge_type')=='fan_out').drop('edge_type')\n",
    "\n",
    "del edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate node attributes from edges\n",
    "# ============== SOURCE ==============\n",
    "\n",
    "window_spec = Window.partitionBy('src')\n",
    "\n",
    "# attr_1, attr_2\n",
    "flow_edges_src = flow_edges.withColumn('sum_amount', F.sum('dst_amount').over(window_spec)) \\\n",
    "                       .withColumn('median_delta', F.median('delta').over(window_spec))\n",
    "flow_edges_src = flow_edges_src.withColumn('att_1', F.round(F.least(F.col('src_amount') / F.col('sum_amount'), F.lit(1)), 6)) \\\n",
    "                       .withColumn('att_2', F.round(F.col('median_delta'), 6)) \\\n",
    "                       .drop('sum_amount', 'median_delta')\n",
    "\n",
    "# attr_3, attr_4\n",
    "fan_in_edges_src = fan_in_edges.withColumn('median_amount', F.median('dst_amount').over(window_spec)) \\\n",
    "                           .withColumn('median_delta', F.median('delta').over(window_spec))\n",
    "fan_in_edges_src = fan_in_edges_src.withColumn('att_3', F.round((F.abs(F.col('src_amount') - F.col('median_amount')) / \n",
    "                                                         F.greatest(F.col('src_amount'), F.col('median_amount'))), 6)) \\\n",
    "                           .withColumn('att_4', F.round(F.col('median_delta'), 6)) \\\n",
    "                           .drop('median_amount', 'median_delta')\n",
    "\n",
    "# attr_5, attr_6\n",
    "fan_out_edges_src = fan_out_edges.withColumn('median_amount', F.median('dst_amount').over(window_spec)) \\\n",
    "                             .withColumn('median_delta', F.median('delta').over(window_spec))\n",
    "fan_out_edges_src = fan_out_edges_src.withColumn('att_5', F.round((F.abs(F.col('src_amount') - F.col('median_amount')) / \n",
    "                                                          F.greatest(F.col('src_amount'), F.col('median_amount'))), 6)) \\\n",
    "                             .withColumn('att_6', F.round(F.col('median_delta'), 6)) \\\n",
    "                             .drop('median_amount', 'median_delta')\n",
    "\n",
    "# Calculate node attributes from edges\n",
    "# ============== TARGET ==============\n",
    "\n",
    "window_spec = Window.partitionBy('dst')\n",
    "\n",
    "# attr_7, attr_8\n",
    "flow_edges_dst = flow_edges.withColumn('sum_amount', F.sum('src_amount').over(window_spec)) \\\n",
    "                       .withColumn('median_delta', F.median('delta').over(window_spec))\n",
    "flow_edges_dst = flow_edges_dst.withColumn('att_7', F.round(F.least(F.col('dst_amount') / F.col('sum_amount'), F.lit(1)), 6)) \\\n",
    "                       .withColumn('att_8', F.round(F.col('median_delta'), 6)) \\\n",
    "                       .drop('sum_amount', 'median_delta')\n",
    "\n",
    "# attr_9, attr_10\n",
    "fan_in_edges_dst = fan_in_edges.withColumn('median_amount', F.median('src_amount').over(window_spec)) \\\n",
    "                           .withColumn('median_delta', F.median('delta').over(window_spec))\n",
    "fan_in_edges_dst = fan_in_edges_dst.withColumn('att_9', F.round((F.abs(F.col('dst_amount') - F.col('median_amount')) / \n",
    "                                                         F.greatest(F.col('dst_amount'), F.col('median_amount'))), 6)) \\\n",
    "                           .withColumn('att_10', F.round(F.col('median_delta'), 6)) \\\n",
    "                           .drop('median_amount', 'median_delta')\n",
    "\n",
    "# attr_11, attr_12\n",
    "fan_out_edges_dst = fan_out_edges.withColumn('median_amount', F.median('src_amount').over(window_spec)) \\\n",
    "                             .withColumn('median_delta', F.median('delta').over(window_spec))\n",
    "fan_out_edges_dst = fan_out_edges_dst.withColumn('att_11', F.round((F.abs(F.col('dst_amount') - F.col('median_amount')) / \n",
    "                                                          F.greatest(F.col('dst_amount'), F.col('median_amount'))), 6)) \\\n",
    "                             .withColumn('att_12', F.round(F.col('median_delta'), 6)) \\\n",
    "                             .drop('median_amount', 'median_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the relevant attributes from edges\n",
    "\n",
    "# ============== SOURCE ==============\n",
    "flow_edges_attr_src = flow_edges_src.select('src', 'att_1', 'att_2').dropDuplicates(['src'])\n",
    "fan_in_edges_attr_src = fan_in_edges_src.select('src', 'att_3', 'att_4').dropDuplicates(['src'])\n",
    "fan_out_edges_attr_src = fan_out_edges_src.select('src', 'att_5', 'att_6').dropDuplicates(['src'])\n",
    "\n",
    "# ============== TARGET ==============\n",
    "flow_edges_attr_dst = flow_edges_dst.select('dst', 'att_7', 'att_8').dropDuplicates(['dst'])\n",
    "fan_in_edges_attr_dst = fan_in_edges_dst.select('dst', 'att_9', 'att_10').dropDuplicates(['dst'])\n",
    "fan_out_edges_attr_dst = fan_out_edges_dst.select('dst', 'att_11', 'att_12').dropDuplicates(['dst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids_col = nodes.select(F.col('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list_src = [\n",
    "    flow_edges_attr_src,\n",
    "    fan_in_edges_attr_src,\n",
    "    fan_out_edges_attr_src\n",
    "]\n",
    "\n",
    "src_attr = reduce(\n",
    "    lambda df1, df2: df1.join(df2, df1.id == df2.src, how=\"left\").drop(\"src\").fillna(0),\n",
    "    edges_list_src,\n",
    "    node_ids_col\n",
    ")\n",
    "\n",
    "edges_list_dst = [\n",
    "    flow_edges_attr_dst,\n",
    "    fan_in_edges_attr_dst,\n",
    "    fan_out_edges_attr_dst\n",
    "]\n",
    "\n",
    "dst_attr = reduce(\n",
    "    lambda df1, df2: df1.join(df2, df1.id == df2.dst, how=\"left\").drop(\"dst\").fillna(0),\n",
    "    edges_list_dst,\n",
    "    node_ids_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_attr_location = f\"../datasets/synthetic/06_gnn_model/{DATASET}_nodes_src\"\n",
    "dst_attr_location = f\"../datasets/synthetic/06_gnn_model/{DATASET}_nodes_dst\"\n",
    "shutil.rmtree(src_attr_location, ignore_errors=True)\n",
    "shutil.rmtree(dst_attr_location, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the attributes 1-6 for the source\n",
    "partitions = 100\n",
    "\n",
    "src_attr.repartition(partitions) \\\n",
    "          .write.mode(\"overwrite\") \\\n",
    "          .parquet(src_attr_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the attributes 7-12 for the target\n",
    "dst_attr.repartition(partitions) \\\n",
    "          .write.mode(\"overwrite\") \\\n",
    "          .parquet(dst_attr_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del flow_edges_attr_src, flow_edges_attr_dst, fan_in_edges_attr_src, fan_in_edges_attr_dst, fan_out_edges_attr_src, fan_out_edges_attr_dst\n",
    "del src_attr, dst_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_attr = spark.read.parquet(src_attr_location)\n",
    "dst_attr = spark.read.parquet(dst_attr_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the node attributes by joining the 12 attributes\n",
    "nodes_attr = src_attr.join(dst_attr, src_attr.id==dst_attr.id,how='inner').drop(dst_attr.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes_location_save = f\"../datasets/synthetic/06_gnn_model/{DATASET}_nodes_gnn\"\n",
    "shutil.rmtree(nodes_location_save, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the node attributes\n",
    "partitions = 200\n",
    "\n",
    "nodes_attr.repartition(partitions) \\\n",
    "          .write.mode(\"overwrite\") \\\n",
    "          .parquet(nodes_location_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nodes_attr, src_attr, dst_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read edges\n",
    "edges_location = f\"../datasets/synthetic/05_temporal_graph/{DATASET}_edges\"\n",
    "edges = spark.read.parquet(edges_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = edges.select(\n",
    "    F.col('src').alias('source'),\n",
    "    F.col('dst').alias('target'),\n",
    "    F.col('weight'),\n",
    "    F.col('delta'),\n",
    "    F.col('edge_type'),\n",
    "    F.col('src_date'),\n",
    "    F.col('dst_date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies [0,1] for edge_type\n",
    "edges = edges.withColumn(\"is_flow\", F.when(F.col(\"edge_type\") == \"flow\", 1).otherwise(0)) \\\n",
    "                    .withColumn(\"is_fan_in\", F.when(F.col(\"edge_type\") == \"fan_in\", 1).otherwise(0)) \\\n",
    "                    .withColumn(\"is_fan_out\", F.when(F.col(\"edge_type\") == \"fan_out\", 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_location_save = f\"../datasets/synthetic/06_gnn_model/{DATASET}_edges_gnn\"\n",
    "partition_by = [\"src_date\", \"dst_date\"]\n",
    "edges.repartition(*partition_by).write.partitionBy(*partition_by).mode(\"overwrite\").parquet(edges_location_save)\n",
    "print(\"edges_written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del edges; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read scores of Isolation Forest (IF) model\n",
    "# select the cutoff point and divide between normal and abnormal transaction ids\n",
    "\n",
    "if_scores_location = f\"../datasets/synthetic/04_if_output/{DATASET}_if_scores.csv\"\n",
    "trans_location = f\"../datasets/synthetic/02_preprocessed/{DATASET}-transactions.parquet\"\n",
    "\n",
    "\n",
    "scores = pd.read_csv(if_scores_location)\n",
    "scores['transaction_id'] = scores['transaction_id'].astype(str)\n",
    "\n",
    "normal_percentage = 70\n",
    "threshold = scores['scores'].quantile(normal_percentage / 100)\n",
    "normal_ids = list(scores[scores['scores'] < threshold]['transaction_id'].values)\n",
    "anomalous_ids = list(scores[scores['scores'] >= threshold]['transaction_id'].values)\n",
    "\n",
    "# Remove anomalous ids from the normal set and reinsert in the anomalous set\n",
    "transactions = pd.read_parquet(trans_location)\n",
    "real_laundering_ids = list(transactions[transactions['is_laundering']==1]['transaction_id'].values)\n",
    "\n",
    "normal_ids_set = set(normal_ids)\n",
    "anomalous_ids_set = set(anomalous_ids)\n",
    "real_laundering_ids_set = set(real_laundering_ids)\n",
    "ids_to_remove = normal_ids_set.intersection(real_laundering_ids_set)\n",
    "\n",
    "normal_ids_set.difference_update(ids_to_remove)\n",
    "anomalous_ids_set.update(ids_to_remove)\n",
    "\n",
    "normal_ids = list(normal_ids_set)\n",
    "anomalous_ids = list(anomalous_ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_location_save = f\"../datasets/synthethic/06_gnn_model/{DATASET}_nodes_gnn\"\n",
    "edges_location_save = f\"../datasets/synthethic/06_gnn_model/{DATASET}_edges_gnn\"\n",
    "\n",
    "nodes = pd.read_parquet(nodes_location_save)\n",
    "edges = pd.read_parquet(edges_location_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = edges[['source', 'target', 'weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a global mapping for node ids, create the entire graph structure using\n",
    "# node features, edge weight and the edge index\n",
    "\n",
    "global_node_mapping = {node_id: idx for idx, node_id in enumerate(nodes['id'])}\n",
    "\n",
    "## =================== ALL GRAPH NODE FEATURES =====================\n",
    "scaler = MinMaxScaler()\n",
    "node_features = nodes.drop(columns=['id']).values \n",
    "all_node_features = scaler.fit_transform(node_features)\n",
    "all_node_features = torch.tensor(all_node_features, dtype=torch.float)\n",
    "\n",
    "## =================== EDGE WEIGHT =====================\n",
    "weight = np.array(edges[['weight']])\n",
    "edge_weight = torch.tensor(weight, dtype=torch.float)\n",
    "\n",
    "## =================== EDGE INDEX ================== \n",
    "edges_ind = edges[['source', 'target']]\n",
    "edge_index_np = np.array([\n",
    "    edges_ind['source'].map(global_node_mapping).values,\n",
    "    edges_ind['target'].map(global_node_mapping).values\n",
    "])\n",
    "edge_index = torch.tensor(edge_index_np, dtype=torch.long)\n",
    "\n",
    "## =================== GRAPH DATA OBJECT ==================\n",
    "data = Data(x=all_node_features, edge_index=edge_index, edge_attr=edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the train node indices as the indices that are associated to normal nodes \n",
    "# Extract the subgraph induced by the normal nodes only\n",
    "\n",
    "train_node_indices = [global_node_mapping[node_id] for node_id in normal_ids]\n",
    "\n",
    "subset = torch.tensor(train_node_indices, dtype=torch.long)\n",
    "edge_index_sub, edge_attr_sub = subgraph(subset, data.edge_index, edge_attr=data.edge_attr, relabel_nodes=True)\n",
    "\n",
    "x_sub = data.x[subset]\n",
    "\n",
    "train_data = Data(x=x_sub, edge_index=edge_index_sub, edge_attr=edge_attr_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate and train GNN-based OD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygod.detector import GAE\n",
    "\n",
    "model = GAE(gpu=0, hid_dim=16, num_layers=2, batch_size=4096, num_neigh=[5,5], epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer on entire graph\n",
    "model_predictions, model_scores = model.predict(data, return_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model scores \n",
    "model_scores = model_scores.numpy() if torch.is_tensor(model_scores) else model_scores\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'id': nodes['id'],\n",
    "    'score': model_scores\n",
    "})\n",
    "\n",
    "results_location = f\"../results/synthetic/{DATASET}_GAE_100_epochs.csv\"\n",
    "results.to_csv(results_location, index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "graph_gnn",
   "name": "common-cu113.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu113:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
