{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Graph of Sequential Transactions\n",
    "*Code adapted from https://github.com/mhaseebtariq/fastman/tree/main*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Instantiate PySpark session\n",
    "config = [\n",
    "    (\"spark.driver.memory\", \"128g\"), \n",
    "    (\"spark.executor.memory\", \"128g\"),\n",
    "    (\"spark.driver.memory\", \"128g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"128g\"),\n",
    "    (\"spark.sql.session.timeZone\", \"UTC\")\n",
    "]\n",
    "spark = SparkSession.builder.appName(\"06_temporal_graph_creation\").config(conf=SparkConf().setAll(config)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = \"HI-Small\"    ## either HI-Small or LI-Small\n",
    "WINDOW = 5              ## time window parameter for connecting transactions in sequence\n",
    "\n",
    "input_path = f\"../datasets/synthetic/02_preprocessed/{DATASET}-transactions.parquet\"\n",
    "DATA_FOLDER = \"../datasets/synthetic/05_temporal_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed transactions with at least ['transaction_id', 'source', 'target', 'timestamp', 'amount'] \n",
    "transactions = pd.read_parquet(input_path)\n",
    "transactions.rename(columns={\"transaction_id\": \"id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location_staging = os.path.join(DATA_FOLDER, f\"{DATASET}_staging\")\n",
    "\n",
    "transactions[\"transaction_timestamp\"] = pd.to_datetime(transactions[\"timestamp\"])\n",
    "transactions[\"transaction_date\"] = transactions[\"transaction_timestamp\"].dt.date\n",
    "transactions[\"transaction_timestamp\"] = transactions[\"transaction_timestamp\"].astype(int) // 10**9\n",
    "del transactions[\"timestamp\"]\n",
    "\n",
    "transactions.to_parquet(location_staging)\n",
    "\n",
    "location_transactions = os.path.join(DATA_FOLDER, f\"{DATASET}_transactions\")\n",
    "staged = spark.read.parquet(location_staging)\n",
    "(\n",
    "    staged.repartition(\"transaction_date\")\n",
    "    .write.partitionBy(\"transaction_date\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(location_transactions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet(location_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.withColumn(\"amount\", sf.ceil(\"amount\").cast(\"long\"))\n",
    "min_timestamp = data.select(sf.min(\"transaction_timestamp\")).collect()[0][0]\n",
    "data = data.withColumn(\"transaction_timestamp\", sf.col(\"transaction_timestamp\") - min_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.orderBy(\"transaction_timestamp\", \"transaction_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rename_columns(dataframe, names):\n",
    "    for name, new_name in names.items():\n",
    "        dataframe = dataframe.withColumnRenamed(name, new_name)\n",
    "    return dataframe\n",
    "\n",
    "def max_timestamp(dt):\n",
    "    year, month, date = dt.split(\"-\")\n",
    "    return (datetime(int(year), int(month), int(date)) + timedelta(days=1)).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the joins of sequential transactions\n",
    "left_columns = {x: f\"{x}_left\" for x in data.columns}\n",
    "dates = data.select(\"transaction_date\").distinct().toPandas()\n",
    "dates = sorted([str(x) for x in dates[\"transaction_date\"].tolist()])\n",
    "\n",
    "location_joins = os.path.join(DATA_FOLDER, f\"{DATASET}_joins\")\n",
    "shutil.rmtree(location_joins, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform a flow join between transactions where a link exist between two nodes\n",
    "# if the target attribute of the first node matches the source of another, within\n",
    "# the specified time window.\n",
    "for transaction_date in dates:\n",
    "    print(transaction_date)\n",
    "    start_index = dates.index(transaction_date)\n",
    "    end_index = start_index + WINDOW + 1\n",
    "    right_dates = dates[start_index:end_index]\n",
    "    end_date_max = str(pd.to_datetime(transaction_date).date() + timedelta(days=WINDOW))\n",
    "    right_dates = [x for x in right_dates if x <= end_date_max]\n",
    "    right = spark.read.option(\"basePath\", location_transactions).parquet(\n",
    "        *[f\"{location_transactions}{os.sep}transaction_date={x}\" for x in right_dates]\n",
    "    )\n",
    "    left = rename_columns(right.where(right.transaction_timestamp < max_timestamp(transaction_date)), left_columns)\n",
    "    flow_join = left.join(right, left.target_left == right.source, \"inner\")\n",
    "    flow_join = flow_join.withColumn(\"delta\", flow_join.transaction_timestamp - flow_join.transaction_timestamp_left)\n",
    "    flow_join = flow_join.where(flow_join.delta > 0)\n",
    "    flow_join.write.parquet(f\"{location_joins}/type=flow/staging_date={transaction_date}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform a fan_in join between transactions where a link exist between two nodes\n",
    "# if the target attribute of the first node matches the target of another, within\n",
    "# the specified time window.\n",
    "for transaction_date in dates:\n",
    "    print(transaction_date)\n",
    "    start_index = dates.index(transaction_date)\n",
    "    end_index = start_index + WINDOW + 1\n",
    "    right_dates = dates[start_index:end_index]\n",
    "    end_date_max = str(pd.to_datetime(transaction_date).date() + timedelta(days=WINDOW))\n",
    "    right_dates = [x for x in right_dates if x <= end_date_max]\n",
    "    right = spark.read.option(\"basePath\", location_transactions).parquet(\n",
    "        *[f\"{location_transactions}{os.sep}transaction_date={x}\" for x in right_dates]\n",
    "    )\n",
    "    left = rename_columns(right.where(right.transaction_timestamp < max_timestamp(transaction_date)), left_columns)\n",
    "    f_in_join = left.join(right, left.target_left == right.target, \"inner\")\n",
    "    f_in_join = f_in_join.withColumn(\"delta\", f_in_join.transaction_timestamp - f_in_join.transaction_timestamp_left)\n",
    "    f_in_join = f_in_join.where(f_in_join.delta > 0)\n",
    "    f_in_join.write.parquet(f\"{location_joins}/type=fan_in/staging_date={transaction_date}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform a fan_out join between transactions where a link exist between two nodes\n",
    "# if the source attribute of the first node matches the source of another, within\n",
    "# the specified time window.\n",
    "for transaction_date in dates:\n",
    "    print(transaction_date)\n",
    "    start_index = dates.index(transaction_date)\n",
    "    end_index = start_index + WINDOW + 1\n",
    "    right_dates = dates[start_index:end_index]\n",
    "    end_date_max = str(pd.to_datetime(transaction_date).date() + timedelta(days=WINDOW))\n",
    "    right_dates = [x for x in right_dates if x <= end_date_max]\n",
    "    right = spark.read.option(\"basePath\", location_transactions).parquet(\n",
    "        *[f\"{location_transactions}{os.sep}transaction_date={x}\" for x in right_dates]\n",
    "    )\n",
    "    left = rename_columns(right.where(right.transaction_timestamp < max_timestamp(transaction_date)), left_columns)\n",
    "    f_out_join = left.join(right, left.source_left == right.source, \"inner\")\n",
    "    f_out_join = f_out_join.withColumn(\"delta\", f_out_join.transaction_timestamp - f_out_join.transaction_timestamp_left)\n",
    "    f_out_join = f_out_join.where(f_out_join.delta > 0)\n",
    "    f_out_join.write.parquet(f\"{location_joins}/type=fan_out/staging_date={transaction_date}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location_joins = os.path.join(DATA_FOLDER, f\"{DATASET}_joins\")\n",
    "joins = spark.read.parquet(location_joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes_location = os.path.join(DATA_FOLDER, f\"{DATASET}_nodes\")\n",
    "edges_location = os.path.join(DATA_FOLDER, f\"{DATASET}_edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the attributes for nodes (transactions) and write nodes\n",
    "node_columns = [\n",
    "    \"id\",\n",
    "    \"source\",\n",
    "    \"target\",\n",
    "    \"transaction_date\",\n",
    "    \"transaction_timestamp\",\n",
    "    \"amount\",\n",
    "]\n",
    "nodes = data.select(*node_columns).drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "nodes = nodes.repartition(\"transaction_date\")\n",
    "nodes.write.partitionBy(\"transaction_date\").mode(\"overwrite\").parquet(nodes_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract edges from the joins, rename the columns and compute weights\n",
    "edges = joins.select(\n",
    "    sf.col(\"id_left\").alias(\"src\"),\n",
    "    sf.col(\"id\").alias(\"dst\"),\n",
    "    sf.col(\"transaction_date_left\").alias(\"src_date\"),\n",
    "    sf.col(\"transaction_date\").alias(\"dst_date\"),\n",
    "    sf.round(\n",
    "        sf.when(\n",
    "            sf.col(\"type\").isin(\"fan_in\", \"fan_out\"), 1\n",
    "        ).otherwise(\n",
    "            sf.when(\n",
    "                sf.col(\"amount_left\") > sf.col(\"amount\"),\n",
    "                sf.col(\"amount\") / sf.col(\"amount_left\")\n",
    "            ).otherwise(\n",
    "                sf.col(\"amount_left\") / sf.col(\"amount\")\n",
    "            )\n",
    "        ), 6\n",
    "    ).alias(\"weight\"),\n",
    "    sf.col(\"type\").alias(\"edge_type\"),\n",
    "    sf.col(\"delta\")\n",
    ")\n",
    "\n",
    "partition_by = [\"src_date\", \"dst_date\"]\n",
    "edges.repartition(*partition_by).write.partitionBy(*partition_by).mode(\"overwrite\").parquet(edges_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nodes = spark.read.parquet(nodes_location)\n",
    "# edges = spark.read.parquet(edges_location)\n",
    "# print(\"# of nodes\", nodes.count())\n",
    "# print(\"# of edges\", edges.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu113:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
