{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "DATASET = \"HI-Small\"    ## either HI-Small or LI-Small\n",
    "MODEL = \"GAE\"           ## either GAE, OCGNN or COLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_location = f\"../results/synthetic/{DATASET}_GAE_100_epochs.csv\"\n",
    "transactions_location = f\"../datasets/synthetic/02_preprocessed/{DATASET}-transactions.parquet\"\n",
    "pattern_location = f\"../datasets/synthetic/02_preprocessed/{DATASET}-patterns.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read gnn model scores, transactions and patterns file (money laundering cases)\n",
    "scores = pd.read_csv(scores_location)\n",
    "transactions = pd.read_parquet(transactions_location)\n",
    "patterns = pd.read_csv(pattern_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure id are compatible across datasets\n",
    "scores[\"id\"] = scores[\"id\"].astype(str)\n",
    "patterns[\"id\"] = patterns[\"id\"].astype(str)\n",
    "patterns[\"transaction_id\"] = patterns[\"transaction_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize gnn model scores in the range [0,1]\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scores['score_min_max_norm'] = min_max_scaler.fit_transform(scores[['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the normalized score with transactions dataframe\n",
    "scores = scores.merge(transactions, left_on=\"id\", right_on=\"transaction_id\", how=\"left\").drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_ids = patterns[['transaction_id', 'id', 'type', 'sub_type']].copy()\n",
    "patterns_ids.rename(columns={'id':'pattern_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the scores dataframe with patterns dataframe\n",
    "scores = scores.merge(patterns_ids, on=\"transaction_id\", how=\"left\").fillna(-1)\n",
    "scores.rename(columns={'score':'model_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the dataframe for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_eval = scores[['transaction_id', 'amount', 'pattern_id', 'type', 'sub_type','model_score', 'score_min_max_norm']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_eval['is_pattern'] = table_eval['pattern_id'].apply(lambda x: 1 if x != -1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read scores of Isolation Forest (IF) model\n",
    "# select the cutoff point and divide between normal and abnormal transaction ids\n",
    "\n",
    "if_scores_location = f\"../datasets/synthetic/04_if_output/{DATASET}_if_scores.csv\"\n",
    "trans_location = f\"../datasets/synthetic/02_preprocessed/{DATASET}-transactions.parquet\"\n",
    "\n",
    "\n",
    "if_scores = pd.read_csv(if_scores_location)\n",
    "if_scores['transaction_id'] = if_scores['transaction_id'].astype(str)\n",
    "\n",
    "normal_percentage = 70\n",
    "threshold = if_scores['scores'].quantile(normal_percentage / 100)\n",
    "normal_ids = list(if_scores[if_scores['scores'] < threshold]['transaction_id'].values)\n",
    "anomalous_ids = list(if_scores[if_scores['scores'] >= threshold]['transaction_id'].values)\n",
    "\n",
    "# Remove anomalous ids from the normal set and reinsert in the anomalous set\n",
    "transactions = pd.read_parquet(trans_location)\n",
    "real_laundering_ids = list(transactions[transactions['is_laundering']==1]['transaction_id'].values)\n",
    "\n",
    "normal_ids_set = set(normal_ids)\n",
    "anomalous_ids_set = set(anomalous_ids)\n",
    "real_laundering_ids_set = set(real_laundering_ids)\n",
    "ids_to_remove = normal_ids_set.intersection(real_laundering_ids_set)\n",
    "\n",
    "normal_ids_set.difference_update(ids_to_remove)\n",
    "anomalous_ids_set.update(ids_to_remove)\n",
    "\n",
    "normal_ids = list(normal_ids_set)\n",
    "anomalous_ids = list(anomalous_ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus the evaluation only on the set of abnormal transactions\n",
    "table_eval = table_eval[table_eval['transaction_id'].isin(anomalous_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the transaction amount to the range [0,1]\n",
    "scaler_amount = MinMaxScaler(feature_range=(0, 1))\n",
    "table_eval['amount_norm'] = scaler_amount.fit_transform(table_eval[['amount']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Combined Score: \n",
    "$ c_i = s_i^{\\textit{norm}} \\times a_i^{\\textit{norm}} $\n",
    "\n",
    "where $s_i$ is the model score for the transaction and $a_i$ is the transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_eval['combined_score'] = table_eval['score_min_max_norm'] * table_eval['amount_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Anomaly Score\n",
    "1. Weight function to target specific amounts, applied to each amount $a_i$\n",
    "\n",
    "    $w(a_i) = \\frac{1}{1 + e^{k(a_i - \\text{threshold})}}$\n",
    "\n",
    "    where $k$ is a parameter controlling the stepness of the weight decay and $threshold$ is the amount threshold we are interested in detecting.\n",
    "    \n",
    "2. Compute anomaly score:\n",
    "\n",
    "    $AnomalyScore_i = c_i \\times w(a_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_function(amount, threshold=35_000, k=0.0001):\n",
    "    return 1 / (1 + np.exp(k * (amount - threshold)))\n",
    "\n",
    "table_eval['amount_weight'] = weight_function(table_eval['amount'])\n",
    "\n",
    "table_eval['combined_score'] = (\n",
    "    (table_eval['score_min_max_norm']) *\n",
    "    (table_eval['amount_norm']) *\n",
    "    table_eval['amount_weight']\n",
    ")\n",
    "\n",
    "# Compute the AUC-ROC with the final anomaly score\n",
    "auc = roc_auc_score(table_eval['is_pattern'], table_eval['combined_score'])\n",
    "print(f\"AUC-ROC with adjusted weights: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final scores \n",
    "table_eval_to_save = table_eval[['is_pattern','combined_score']]\n",
    "table_eval_to_save.to_csv(f\"{DATASET}_{MODEL}_final_score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for all models on the two datasets\n",
    "\n",
    "DATASETS = [\"HI-Small\", \"LI-Small\"]\n",
    "MODELS = [\"GAE\", \"OCGNN\", \"CoLA\"]\n",
    "\n",
    "for model in MODELS:\n",
    "    # Load and rename scores for HI-Small and LI-Small datasets\n",
    "    hi_small_file = f\"{DATASETS[0]}_{model}_final_score.csv\"\n",
    "    li_small_file = f\"{DATASETS[1]}_{model}_final_score.csv\"\n",
    "    \n",
    "    hi_table = pd.read_csv(hi_small_file)\n",
    "    li_table = pd.read_csv(li_small_file)\n",
    "    hi_table.rename(columns={'combined_score': f'{model}_score'}, inplace=True)\n",
    "    li_table.rename(columns={'combined_score': f'{model}_score'}, inplace=True)\n",
    "\n",
    "    # Calculate the top 30% threshold\n",
    "    threshold_70_hi = hi_table[f'{model}_score'].quantile(0.7)\n",
    "    threshold_70_li = li_table[f'{model}_score'].quantile(0.7)\n",
    "\n",
    "    # Assign predictions based on threshold\n",
    "    hi_table['predicted'] = (hi_table[f'{model}_score'] >= threshold_70_hi).astype(int)\n",
    "    li_table['predicted'] = (li_table[f'{model}_score'] >= threshold_70_li).astype(int)\n",
    "\n",
    "    # Calculate confusion matrices\n",
    "    conf_matrix_hi = confusion_matrix(hi_table['is_pattern'], hi_table['predicted'])\n",
    "    conf_matrix_li = confusion_matrix(li_table['is_pattern'], li_table['predicted'])\n",
    "\n",
    "    # Extract TN, FP, FN, TP from the confusion matrix\n",
    "    TN_hi, FP_hi, FN_hi, TP_hi = conf_matrix_hi.ravel()\n",
    "    TN_li, FP_li, FN_li, TP_li = conf_matrix_li.ravel()\n",
    "\n",
    "    # Create confusion matrices in the specified layout (with TP in the top-left)\n",
    "    confusion_matrix_hi_small = np.array([[TP_hi, FN_hi], [FP_hi, TN_hi]])\n",
    "    confusion_matrix_li_small = np.array([[TP_li, FN_li], [FP_li, TN_li]])\n",
    "\n",
    "    # Set up the plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot HI-Small Confusion Matrix\n",
    "    sns.heatmap(confusion_matrix_hi_small, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "                yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    "                ax=axes[0])\n",
    "    axes[0].set_title(f\"{DATASETS[0]} Dataset - {model} Model - Confusion Matrix\")\n",
    "    axes[0].set_xlabel(\"Predicted Labels\")\n",
    "    axes[0].set_ylabel(\"Actual Labels\")\n",
    "\n",
    "    # Plot LI-Small Confusion Matrix\n",
    "    sns.heatmap(confusion_matrix_li_small, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=[\"Predicted Positive\", \"Predicted Negative\"],\n",
    "                yticklabels=[\"Actual Positive\", \"Actual Negative\"],\n",
    "                ax=axes[1])\n",
    "    axes[1].set_title(f\"{DATASETS[1]} Dataset - {model} Model - Confusion Matrix\")\n",
    "    axes[1].set_xlabel(\"Predicted Labels\")\n",
    "    axes[1].set_ylabel(\"Actual Labels\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the written final scores\n",
    "\n",
    "DATASET = \"HI-Small\"\n",
    "\n",
    "gae_location = f\"{DATASET}_GAE_final_score.csv\"\n",
    "ocgnn_location = f\"{DATASET}_OCGNN_final_score.csv\"\n",
    "cola_location = f\"{DATASET}_CoLA_final_score.csv\"\n",
    "\n",
    "gae_table = pd.read_csv(gae_location)\n",
    "ocgnn_table = pd.read_csv(ocgnn_location)\n",
    "cola_table = pd.read_csv(cola_location)\n",
    "\n",
    "gae_table.rename(columns={'combined_score': 'GAE_score'}, inplace=True)\n",
    "ocgnn_table.rename(columns={'combined_score': 'OCGNN_score'}, inplace=True)\n",
    "cola_table.rename(columns={'combined_score': 'COLA_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot auc-roc comparison curve for all models on the two datasets\n",
    "\n",
    "fpr_GAE, tpr_GAE, _ = roc_curve(gae_table['is_pattern'], gae_table['GAE_score'])\n",
    "auc_GAE = roc_auc_score(gae_table['is_pattern'], gae_table['GAE_score'])\n",
    "\n",
    "fpr_OCGNN, tpr_OCGNN, _ = roc_curve(ocgnn_table['is_pattern'], ocgnn_table['OCGNN_score'])\n",
    "auc_OCGNN = roc_auc_score(ocgnn_table['is_pattern'], ocgnn_table['OCGNN_score'])\n",
    "\n",
    "fpr_CoLA, tpr_CoLA, _ = roc_curve(cola_table['is_pattern'], cola_table['CoLA_score'])\n",
    "auc_CoLA = roc_auc_score(cola_table['is_pattern'], cola_table['CoLA_score'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_GAE, tpr_GAE, label=f'GAE (AUC = {auc_GAE:.2f})')\n",
    "plt.plot(fpr_OCGNN, tpr_OCGNN, label=f'OCGNN (AUC = {auc_OCGNN:.2f})')\n",
    "plt.plot(fpr_CoLA, tpr_CoLA, label=f'CoLA (AUC = {auc_CoLA:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
    "\n",
    "plt.xlabel('False Positive Rate (FP Rate)')\n",
    "plt.ylabel('True Positive Rate (TP Rate)')\n",
    "plt.title(f'{DATASET} Dataset AUC-ROC Curve Comparison for Different Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(f\"{DATASET}_auc.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print other metrics\n",
    "\n",
    "table_eval = table_eval.sort_values(by='combined_score', ascending=False)\n",
    "\n",
    "top_n = 100_000\n",
    "\n",
    "table_eval['predicted_label'] = 0\n",
    "table_eval.iloc[:top_n, table_eval.columns.get_loc('predicted_label')] = 1\n",
    "\n",
    "y_true = table_eval['is_pattern']\n",
    "y_pred = table_eval['predicted_label']\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\n",
    "\n",
    "confusion_df = pd.DataFrame([[tn, fp], [fn, tp]],\n",
    "                            index=['Actual Normal', 'Actual Anomaly'],\n",
    "                            columns=['Predicted Normal', 'Predicted Anomaly'])\n",
    "print(confusion_df)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall: {recall*100:.2f}%\")\n",
    "print(f\"F1 Score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = table_eval['is_pattern']\n",
    "y_scores = table_eval['combined_score']\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f\"AUC-ROC with adjusted weights: {roc_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw scores of the gnn based models\n",
    "\n",
    "DATASET = \"HI-Small\"\n",
    "f\"../results/synthetic/{DATASET}_GAE_100_epochs.csv\"\n",
    "gae_scores_loc = f\"../results/synthetic/{DATASET}_GAE_100_epochs.csv\"\n",
    "ocgnn_scores_loc = f\"../results/synthetic/{DATASET}_OCGNN_100_epochs.csv\"\n",
    "cola_scores_loc = f\"../results/synthetic/{DATASET}_CoLA_100_epochs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_scores = pd.read_csv(gae_scores_loc)\n",
    "ocgnn_scores = pd.read_csv(ocgnn_scores_loc)\n",
    "cola_scores = pd.read_csv(cola_scores_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_scores_location = f\"../datasets/synthetic/04_if_output/{DATASET}_if_scores.csv\"\n",
    "trans_location = f\"../datasets/synthetic/02_preprocessed/{DATASET}-transactions.parquet\"\n",
    "\n",
    "if_scores = pd.read_csv(if_scores_location)\n",
    "if_scores['transaction_id'] = if_scores['transaction_id'].astype(str)\n",
    "\n",
    "normal_percentage = 70\n",
    "threshold = if_scores['scores'].quantile(normal_percentage / 100)\n",
    "normal_ids = list(if_scores[if_scores['scores'] < threshold]['transaction_id'].values)\n",
    "anomalous_ids = list(if_scores[if_scores['scores'] >= threshold]['transaction_id'].values)\n",
    "\n",
    "transactions = pd.read_parquet(trans_location)\n",
    "real_laundering_ids = list(transactions[transactions['is_laundering']==1]['transaction_id'].values)\n",
    "\n",
    "normal_ids_set = set(normal_ids)\n",
    "anomalous_ids_set = set(anomalous_ids)\n",
    "real_laundering_ids_set = set(real_laundering_ids)\n",
    "ids_to_remove = normal_ids_set.intersection(real_laundering_ids_set)\n",
    "\n",
    "normal_ids_set.difference_update(ids_to_remove)\n",
    "anomalous_ids_set.update(ids_to_remove)\n",
    "\n",
    "normal_ids = list(normal_ids_set)\n",
    "anomalous_ids = list(anomalous_ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_scores[\"id\"] = gae_scores[\"id\"].astype(str)\n",
    "ocgnn_scores[\"id\"] = ocgnn_scores[\"id\"].astype(str)\n",
    "cola_scores[\"id\"] = cola_scores[\"id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_scores = gae_scores[gae_scores['id'].isin(anomalous_ids)]\n",
    "ocgnn_scores = ocgnn_scores[ocgnn_scores['id'].isin(anomalous_ids)]\n",
    "cola_scores = cola_scores[cola_scores['id'].isin(anomalous_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "gae_scores[\"norm_score\"] = scaler.fit_transform(gae_scores[[\"score\"]])\n",
    "ocgnn_scores[\"norm_score\"] = scaler.fit_transform(ocgnn_scores[[\"score\"]])\n",
    "cola_scores[\"norm_score\"] = scaler.fit_transform(cola_scores[[\"score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of scores of the GNN models before post-processing the score\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.histplot(gae_scores['norm_score'], bins=50, kde=False, color='blue', label='GAE model scores', stat='density', alpha=0.5)\n",
    "sns.histplot(ocgnn_scores['norm_score'], bins=50, kde=False, color='green', label='OCGNN model scores', stat='density', alpha=0.5)\n",
    "sns.histplot(cola_scores['norm_score'], bins=50, kde=False, color='red', label='CoLA model scores', stat='density', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Normalized Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'{DATASET} - Raw Scores Distribution on the Inference Subset')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{DATASET}_scores.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "graph_gnn",
   "name": "common-cu113.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu113:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
