{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Data Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "input_data_path = \"../datasets/real/Real-Agg_Trans.csv\"\n",
    "results_path = \"../results/Real-Agg_scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv(input_data_path, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.rename(\n",
    "    columns={'start_id': 'source',\n",
    "             'end_id': 'target',\n",
    "             'total': 'amount'})\n",
    "transactions[\"transaction_id\"] = transactions.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Weakly Connected Components (WCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = transactions.loc[:, ['source', 'target', 'amount', 'year_from', 'year_to', 'count', 'transaction_id']]\n",
    "\n",
    "graph = ig.Graph.DataFrame(edges, use_vids=False, directed=True)\n",
    "all_nodes = pd.DataFrame([x[\"name\"] for x in graph.vs()], columns=[\"name\"])\n",
    "\n",
    "weak_components = sorted(\n",
    "    [(x, len(x)) for x in graph.components(mode=\"weak\")], reverse=True, key=lambda x: x[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nodes = graph.vcount()\n",
    "total_edges = graph.ecount()\n",
    "\n",
    "weak_components = sorted(graph.components(mode=\"weak\"), key=len, reverse=True)\n",
    "\n",
    "top_components = []\n",
    "for i, component in enumerate(weak_components[:3], start=1):\n",
    "    num_nodes = len(component)\n",
    "    num_edges = graph.subgraph(component).ecount()\n",
    "    \n",
    "    top_components.append({\n",
    "        \"Component\": i,\n",
    "        \"# Nodes\": num_nodes,\n",
    "        \"% of Total Nodes\": f\"{(num_nodes / total_nodes) * 100:.4f}%\",\n",
    "        \"# Edges\": num_edges,\n",
    "        \"% of Total Edges\": f\"{(num_edges / total_edges) * 100:.4f}%\"\n",
    "    })\n",
    "\n",
    "top_components_df = pd.DataFrame(top_components)\n",
    "print(top_components_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only the largest WCC\n",
    "largest_weak_component = weak_components[0]\n",
    "largest_component_nodes = set(graph.vs[largest_weak_component][\"name\"])\n",
    "\n",
    "filtered_transactions = edges[\n",
    "    edges[\"source\"].isin(largest_component_nodes) & edges[\"target\"].isin(largest_component_nodes)\n",
    "]\n",
    "transactions = filtered_transactions.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['years'] = transactions['year_to'] - transactions['year_from'] + 1\n",
    "transactions['amount_per_year'] = transactions['amount'] / transactions['years']\n",
    "transactions['count_per_year'] = transactions['count'] / transactions['years']\n",
    "\n",
    "sub_df = transactions[[\"source\", \"target\", \"amount\"]]\n",
    "\n",
    "src_group = sub_df.groupby(\"source\").agg(\n",
    "    total_sent = (\"amount\", \"sum\"),\n",
    "    avg_sent = (\"amount\", \"mean\"),\n",
    "    stddev_sent = (\"amount\", \"std\"),\n",
    "    src_total_counterparties = (\"target\", \"nunique\"),\n",
    ").reset_index()\n",
    "\n",
    "dst_group = sub_df.groupby(\"target\").agg(\n",
    "    total_received = (\"amount\", \"sum\"),\n",
    "    avg_received = (\"amount\", \"mean\"),\n",
    "    stddev_received = (\"amount\", \"std\"),\n",
    "    dst_total_counterparties = (\"source\", \"nunique\"),\n",
    ").reset_index()\n",
    "\n",
    "transactions = transactions.merge(src_group, on=[\"source\"], how=\"left\")\n",
    "transactions = transactions.merge(dst_group, on=[\"target\"], how=\"left\")\n",
    "\n",
    "transactions['percentage_of_total_sent'] = (transactions['amount'] / transactions['total_sent']) * 100\n",
    "transactions['percentage_of_total_received'] = (transactions['amount'] / transactions['total_received']) * 100\n",
    "transactions['percentage_of_avg_sent'] = (transactions['amount'] / transactions['avg_sent']) * 100\n",
    "transactions['percentage_of_avg_received'] = (transactions['amount'] / transactions['avg_received']) * 100\n",
    "\n",
    "total_interactions = transactions.groupby(['source', 'target']).size().reset_index(name='total_interactions')\n",
    "transactions = transactions.merge(total_interactions, how='left', on=['source', 'target'])\n",
    "del total_interactions\n",
    "\n",
    "src_interactions = transactions.groupby('source').size().reset_index(name='src_interactions')\n",
    "dst_interactions = transactions.groupby('target').size().reset_index(name='dst_interactions')\n",
    "\n",
    "transactions = transactions.merge(src_interactions, on='source', how='left')\n",
    "transactions = transactions.merge(dst_interactions, on='target', how='left')\n",
    "\n",
    "transactions['counterparty_diversity'] = transactions.groupby('source')['target'].transform('nunique') / transactions['total_interactions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['transaction_frequency'] = (\n",
    "    transactions['total_interactions'] / transactions['years']\n",
    ")\n",
    "\n",
    "transactions['transaction_amount_variance'] = (\n",
    "    transactions['stddev_sent'] / (transactions['avg_sent'] + 1e-10)\n",
    ")\n",
    "\n",
    "transactions['transaction_ratio'] = (\n",
    "    transactions['total_sent'] / (transactions['total_received'] + 1e-10)\n",
    ")\n",
    "\n",
    "transactions['unique_counterparty_ratio'] = (\n",
    "    (transactions['src_total_counterparties'] + transactions['dst_total_counterparties']) \n",
    "    / (transactions['total_interactions'] + 1e-10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.from_pandas_edgelist(transactions, source='source', target='target', create_using=nx.DiGraph())\n",
    "\n",
    "degree_centrality = nx.degree_centrality(graph)\n",
    "pagerank_scores = nx.pagerank(graph)\n",
    "\n",
    "transactions['source_degree_centrality'] = transactions['source'].map(degree_centrality)\n",
    "transactions['target_degree_centrality'] = transactions['target'].map(degree_centrality)\n",
    "transactions['source_pagerank'] = transactions['source'].map(pagerank_scores)\n",
    "transactions['target_pagerank'] = transactions['target'].map(pagerank_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'amount', 'count', 'years', 'amount_per_year', 'count_per_year', \n",
    "    'total_sent', 'avg_sent', 'stddev_sent', 'total_received', \n",
    "    'avg_received', 'stddev_received', 'src_total_counterparties', \n",
    "    'dst_total_counterparties', 'counterparty_diversity', \n",
    "    'percentage_of_total_sent', 'percentage_of_total_received', \n",
    "    'percentage_of_avg_sent', 'percentage_of_avg_received', \n",
    "    'transaction_ratio', 'transaction_frequency', \n",
    "    'transaction_amount_variance', 'unique_counterparty_ratio',\n",
    "    'source_degree_centrality', 'target_degree_centrality',\n",
    "    'source_pagerank', 'target_pagerank']\n",
    "X_ids = transactions[['transaction_id', 'amount']]\n",
    "X_train = transactions[selected_features]\n",
    "X_train = X_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_train.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix of Selected Features\")\n",
    "plt.show()\n",
    "\n",
    "high_correlation_pairs = [\n",
    "    (col1, col2) for col1 in correlation_matrix.columns for col2 in correlation_matrix.columns \n",
    "    if col1 != col2 and abs(correlation_matrix.loc[col1, col2]) > 0.9\n",
    "]\n",
    "\n",
    "features_to_drop = set()\n",
    "for col1, col2 in high_correlation_pairs:\n",
    "    features_to_drop.add(col2)\n",
    "\n",
    "X_train_reduced = X_train.drop(columns=features_to_drop)\n",
    "\n",
    "print(\"\\nReduced features after removing highly correlated pairs:\")\n",
    "print(X_train_reduced.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "trees= 100\n",
    "samples = 0.01\n",
    "jobs = -1\n",
    "state = 42\n",
    "\n",
    "model = IsolationForest(n_estimators=trees,\n",
    "                        max_samples=samples,\n",
    "                        n_jobs=jobs,\n",
    "                        random_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the IF model\n",
    "model.fit(X_train_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform inference and export predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained IF model to compute anomaly scores for the training data\n",
    "scores = model.decision_function(X_train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert and scale Isolation Forest scores to [0, 1], where higher scores indicate anomalies\n",
    "inverted_scores = -scores\n",
    "inverted_scores = (inverted_scores - inverted_scores.min()) / (inverted_scores.max() - inverted_scores.min())\n",
    "scaled_scores = inverted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.concat([X_ids, X_train_reduced], axis=1)\n",
    "evaluation[\"scores\"] = scaled_scores\n",
    "\n",
    "evaluation = evaluation.sort_values(by='scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 90% percentile as threshold to separate between normal and abnomal transactions\n",
    "THRESHOLD = 0.9\n",
    "threshold = evaluation['scores'].quantile(THRESHOLD)\n",
    "\n",
    "prediction = [1 if score>=threshold else 0 for score in evaluation['scores']]\n",
    "evaluation['prediction'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results dataframe\n",
    "results = evaluation[['transaction_id', 'amount', 'count', 'years', 'total_sent', 'scores', 'prediction']].copy()\n",
    "results.to_csv(results_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataframe containing scores and prediction labels\n",
    "results = pd.read_csv(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_results = results.sample(frac=0.5, random_state=42)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='amount', y='scores', data=sampled_results, hue='prediction', palette='viridis', s=50, edgecolor='k', alpha=0.7, legend=False)\n",
    "\n",
    "plt.title(\"Scores vs Transaction Amount\")\n",
    "plt.xlabel(\"Transaction Amount\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"results_real.csv\")\n",
    "results = results.merge(transactions, on=\"transaction_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous = results[results['prediction']==1]\n",
    "normal = results[results['prediction']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total amount of transactions identified normal \", normal['amount_x'].sum())\n",
    "print(\"Total amount of transactions identified anomalous \", anomalous['amount_x'].sum())\n",
    "\n",
    "print(\"Median amount  of transactions identified normal \", normal[\"amount_x\"].median())\n",
    "print(\"Median amount  of transactions identified anomalous \", anomalous[\"amount_x\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakly Connected Components from anomalous transactions graph only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = results.loc[:, ['source', 'target', 'transaction_id', 'prediction']]\n",
    "edges = edges[edges['prediction']==1]\n",
    "\n",
    "graph = ig.Graph.DataFrame(edges, use_vids=False, directed=True)\n",
    "\n",
    "total_nodes = graph.vcount()\n",
    "total_edges = graph.ecount()\n",
    "\n",
    "weak_components = sorted(graph.components(mode=\"weak\"), key=len, reverse=True)\n",
    "\n",
    "top_components = []\n",
    "for i, component in enumerate(weak_components[:3], start=1):\n",
    "    num_nodes = len(component)\n",
    "    num_edges = graph.subgraph(component).ecount()\n",
    "    \n",
    "    top_components.append({\n",
    "        \"Component\": i,\n",
    "        \"# Nodes\": num_nodes,\n",
    "        \"% of Total Nodes\": f\"{(num_nodes / total_nodes) * 100:.4f}%\",\n",
    "        \"# Edges\": num_edges,\n",
    "        \"% of Total Edges\": f\"{(num_edges / total_edges) * 100:.4f}%\"\n",
    "    })\n",
    "\n",
    "top_components_df = pd.DataFrame(top_components)\n",
    "print(top_components_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
